\section{Research Contributions}
\label{sec:research-contributions}

\par This research provides several key contributions in the area of personalized emotion recognition and emotionally aware AI systems. The focus was on building a system that works differently for each user by understanding their unique emotional patterns through both facial and vocal signals.

\begin{itemize}
    \item \paragraph*{Personalized Emotion Recognition using MSE-based Fusion} One of the main contributions is the implementation of a personalized multimodal emotion recognition method. This was done by combining facial and vocal predictions using a decision-level fusion approach. The weights were calculated based on the MSE between the predicted values and user self-reported emotions. This personalized approach was applied for identifying five target emotions: Happy, Angry, Sad, Boredom, and Calm.

    \item \paragraph*{Emotional Baseline Identification in Arousal-Valence Plane} Another key contribution is the method used to identify each user's emotional baseline in the arousal-valence space. KDE was used on the collected data to estimate a stable starting point for each participantâ€™s emotional state.

    \item \paragraph*{Baseline Refinement using Emoji-Based Feedback Mechanism} A novel method was introduced to refine the baseline over time using user feedback. Instead of traditional methods, participants gave feedback using simple emoji-based inputs. These responses were used in a reinforcement learning loop to adjust the emotional baseline.

\end{itemize}

\par These contributions together help to create a more personalized and emotionally intelligent interaction between users and AI systems. They also offer a foundation for future work in real-time and long-term emotion tracking systems for individuals.
