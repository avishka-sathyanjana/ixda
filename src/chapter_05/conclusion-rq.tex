\section{Conclusions about the Research Questions and Aim}
\label{sec:conclusion-rq}

\par The main aim of this research was to develop a personalized emotion recognition system using facial and vocal signals, and to integrate these emotional insights into LLM responses to make them more emotionally intelligent. The research was divided into five stages, and each stage focused on addressing specific research questions and objectives.

\par In the first stage, several pre-implemented facial and vocal emotion recognition models were compared to answer \hyperref[rq:1.1]{RQ 1.1} (What are suitable pre-implemented models that can be used to get a higher accuracy for emotion recognition?). Between HUME and CAGE for facial expression, HUME scored considerably higher overall. In the case of audio models, both HUME Audio and Wave2Vec2 showed different strengths, but the performance gap between them was smaller compared to facial models. We selected the HUME model for both modalities to maintain consistency. This model selection helped to lay a strong foundation for building a multimodal framework.

\par In the second stage, we addressed \hyperref[rq:1.2]{RQ 1.2} (How the recognized emotion values from different modalities fused together in order to get more personalized arousal-valence value?) by fusing the facial and vocal predictions using decision-level fusion. Mean Squared Error was used to calculate the weights for fusion. The fused predictions showed lower average Euclidean distance compared to using either facial or vocal data alone, especially in cases where one modality was weaker. This means the fused model was more accurate and closely matched the ground truth emotions.

\par The third stage focused on identifying an initial emotional baseline to answer \hyperref[rq:2.1]{RQ 2.1} (What techniques are most suitable for establishing an initial emotional baseline and how can this baseline be dynamically adjusted over time to reflect changes in the user's emotional responses and self-reported feedback?). We used Kernel Density Estimation based on participant data. To verify accuracy, we collected feedback from participants through a questionnaire. The average agreement score was 3.70 out of 5, showing that most users agreed with the identified baseline values.
 
\par In the fourth stage, we evaluated how emotional input could change the output of a LLM model. We used GPT-4o-mini and found a noticeable increase in user satisfaction when emotional context was included with user queries. This confirms the value of emotionally aware responses and successfully answers \hyperref[rq:3.1]{RQ 3.1} (How does integrating personalized emotional state information with user queries affect the relevance and user satisfaction of responses from LLMs?).

\par Finally, in the fifth stage, we revisited the emotional baseline and used reinforcement learning to refine it over time. Among six participants, four rated the refined baseline with 4 or higher out of 5, giving a 66.67\% agreement rate. This result shows that the method works for most users and improves personalization over time, again supporting \hyperref[rq:2.1]{RQ 2.1}.

\par Overall, this research shows that using personalized emotional state recognition with facial and audio data can meaningfully improve how LLMs respond to users. The findings give a starting point for building emotionally aware AI systems and highlight areas for future improvements like larger datasets, longer-term user adaptation, and better real-time emotion tracking.
