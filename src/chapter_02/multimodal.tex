\section{Multi-modal Fusion}
\par Multimodal analysis leverages input data from various channels like video, audio, and text to enhance the performance and accuracy of emotion recognition systems. The fusion of this multi-modal data is crucial, with techniques including feature-level fusion (combining features into one vector), decision-level fusion (independently classifying features and fusing outcomes), hybrid fusion (combining feature and decision-level approaches), model-level fusion (using correlations between models), rule-based fusion (assigning normalized weights), classification-based fusion (employing algorithms like SVMs and neural networks), and estimation-based fusion (useful for real-time audio and visual data, with filters like Kalman and particle filters). These fusion methods aim to effectively combine the data gathered from multiple modalities, enabling better emotional classification and recognition \citep{poria2017review}.


In summary, emotion recognition research has evolved from foundational psychological theories like Plutchik’s wheel and the VAD model to sophisticated multimodal systems that analyze facial, vocal, and textual cues. FER models such as EmotiEffNet and MaxViT demonstrate strong performance in valence-arousal prediction, balancing accuracy with computational efficiency. Hume.ai’s approach offers a culturally diverse and highly detailed understanding of emotional expression. Similarly, SER leverages CNNs, LSTMs, and transformers to model emotional variations in audio, with recent advances achieving state-of-the-art results using large, diverse datasets. Together, these developments underscore the importance of multimodal, culturally-aware, and continuously annotated datasets for advancing emotion recognition technologies.